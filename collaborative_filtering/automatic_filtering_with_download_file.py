# -*- coding: utf-8 -*-
"""Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J1Tk20TL0Yzj23753mGX1HhxcPcBNxhV
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate
from tensorflow.keras.models import Model
from scipy.spatial.distance import cosine
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

# Mount drive to access dataset
from google.colab import drive
drive.mount('/content/gdrive')

# Load the dataset
data = pd.read_csv('gdrive/My Drive/Capstone Project/ML Code/jgo_data_merge.csv') # Load from Drive
data.head()

# Get user ID, user ratings, and place name data
data = data.groupby(['user_id', 'place_name'], as_index=False).agg({
    'user_rate': 'mean' # Find mean of user ratings for each place to avoid duplicate entries
    })

# Converting data from long to wide format
data_wide = data.pivot(index="user_id",columns="place_name",values="user_rate")
data_wide.head()

# Replace NaN data with 0
data_wide.fillna(0, inplace=True)
data_wide.head()

# Drop the user column in different DF
data_tourismbased = data_wide.copy()
data_tourismbased = data_tourismbased.reset_index()
data_tourismbased = data_tourismbased.drop("user_id", axis=1)
data_tourismbased.head()

# Create a dataframe for place data (place vs place) to find relations
tourismbased = pd.DataFrame(index=data_tourismbased.columns,
                            columns=data_tourismbased.columns)
tourismbased.head()

# Calculate similarity between places
for i in range(0,len(tourismbased.columns)) :
    # Loop through the columns for each column
    for j in range(0,len(tourismbased.columns)) :
      # Fill in placeholder with cosine similarities
      tourismbased.iloc[i,j] = 1-cosine(data_tourismbased.iloc[:,i],data_tourismbased.iloc[:,j])

tourismbased.head()

# Looking for neighbour data based on the similarity matrix
data_neighbours = pd.DataFrame(index=tourismbased.columns,columns=range(1,11))

# Loop through our similarity dataframe and fill in neighbouring place names
for i in range(0,len(tourismbased.columns)):
    data_neighbours.iloc[i,:10] = tourismbased.iloc[0:,i].sort_values(ascending=False)[:10].index

data_neighbours

# Helper function to get similarity scores
def getScore(history, similarities):
   return sum(history*similarities)/sum(similarities)

# Reset data index to start from 0
data_userbased1 = data_wide.reset_index()
data_userbased1.head()

# Create a place holder matrix for similarities, and fill in the user name column
userbased = pd.DataFrame(index=data_userbased1.index,columns=data_userbased1.columns)
userbased.iloc[:,:1] = data_userbased1.iloc[:,:1]
userbased.head()

# New DF for the first 540 rows and all columns of the data_userbased1 DF
data_userbased12 = data_userbased1.iloc[:540,:]

# New DF for the first 540 rows and all columns of the userbased DF
data_userbased11 = userbased.iloc[:540,:]

# Iterate through each data in the matrix
for i in range(0,len(data_userbased11.index)):
    for j in range(1,len(data_userbased11.columns)):

        # Get the current user and tourism (place)
        user = data_userbased11.index[i]
        tourism = data_userbased11.columns[j]

        # If the user has already rated the place, set the predicted rating to 0
        if data_userbased12.iloc[i][j] == 1:
            data_userbased11.iloc[i][j] = 0
        # If the user has not rated the place, predict the rating
        else:
            # Get the top 10 similar places to the current tourism
            tourism_top_names = data_neighbours.loc[tourism][1:10]
            # Get the similarity scores between the current tourism and its top 10 similar places
            tourism_top_sims = tourismbased.loc[tourism].sort_values(ascending=False)[1:10]
            # Get the ratings the user has given to those similar places
            user_rated = data_tourismbased.loc[user,tourism_top_names]

            # Calculate the predicted rating using the getScore function
            data_userbased11.iloc[i][j] = getScore(user_rated,tourism_top_sims)

# Get the top places for user
data_recommend = pd.DataFrame(index=userbased.index, columns=['user_id','1','2','3','4','5','6'])
data_recommend.iloc[0:,0] = userbased.iloc[:,0]

# Instead of top places scores, show names
for i in range(0,len(userbased.index)):
    data_recommend.iloc[i,1:] = userbased.iloc[i,:].sort_values(ascending=False).iloc[1:7,].index.transpose()

# Print a sample
print (data_recommend.iloc[:10,:4])

# Generate training data
train_users = []
train_places = []
train_ratings = []

# Create a mapping from place names to numerical indices
item_index = {tourism: i for i, tourism in enumerate(data_userbased11.columns[1:])}

for i in range(len(data_userbased11.index)):
    for j in range(1, len(data_userbased11.columns)):
        user = data_userbased11.index[i]
        tourism = data_userbased11.columns[j]
        score = data_userbased11.iloc[i, j]

        # Only include meaningful scores in training
        # Check if the score is not NaN and is a valid number
        if not np.isnan(score) and np.isfinite(score):
            train_users.append(user)
            # Use the numerical index instead of the place name
            train_places.append(item_index[tourism])
            train_ratings.append(score)

# Convert to numpy arrays
train_users = np.array(train_users)
train_places = np.array(train_places)
train_ratings = np.array(train_ratings)

# Number of users and places
n_users = len(data_userbased11.index)
n_places = len(data_userbased11.columns) - 1

# Define inputs
user_input = Input(shape=(1,))
item_input = Input(shape=(1,))

# Embeddings for users and places
user_embedding = Embedding(input_dim=n_users, output_dim=50)(user_input)
item_embedding = Embedding(input_dim=n_places, output_dim=50)(item_input)

# Flatten embeddings
user_vec = Flatten()(user_embedding)
item_vec = Flatten()(item_embedding)

# Combine embeddings
x = Concatenate()([user_vec, item_vec])

# Fully connected layers
x = Dense(128, activation='relu')(x)
x = Dense(64, activation='relu')(x)
output = Dense(1)(x)

# Compile the model
model = Model(inputs=[user_input, item_input], outputs=output)
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# Train the model
history = model.fit([train_users, train_places], train_ratings, epochs=10, batch_size=32, validation_split=0.2)

# Evaluate the model
predicted_ratings = model.predict([train_users, train_places])

# Remove NaN values from predicted ratings before calculation
# This step is added as a precaution in case the model predicts NaNs
mask = np.isfinite(predicted_ratings.flatten())
predicted_ratings = predicted_ratings[mask]
train_ratings = train_ratings[mask]

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(train_ratings, predicted_ratings))

# Calculate MAE
mae = mean_absolute_error(train_ratings, predicted_ratings)

print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"Mean Absolute Error (MAE): {mae}")

# Interpreting the results:
# RMSE: It represents the average difference between the predicted ratings and the actual ratings.
# A lower RMSE indicates better accuracy.
# Ideally, you want RMSE to be below 1 in a rating prediction scenario where ratings are typically on a scale of 1 to 5.
# MAE: It is the average absolute difference between the predicted and actual ratings.
# It is less sensitive to outliers compared to RMSE. A lower MAE also indicates better accuracy.
# Similar to RMSE, a MAE value below 1 is generally considered good for a 1-5 rating scale.

from google.colab import files

# Save model to H5 file
# files.download('gdrive/My Drive/Capstone Project/ML Code/jgo.h5')

# Save model to Pickle file
# import pickle
# with open('gdrive/My Drive/Capstone Project/ML Code/jgo.pkl', 'wb') as f:
#     pickle.dump(model, f)
# files.download('gdrive/My Drive/Capstone Project/ML Code/jgo.pkl')